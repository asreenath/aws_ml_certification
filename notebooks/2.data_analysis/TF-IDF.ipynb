{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29773dab",
   "metadata": {},
   "source": [
    "Initialize Spark Context as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e8bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('TestTFIDF').getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96490230-2866-4102-9ca2-dd4516309182",
   "metadata": {},
   "source": [
    "Let's start by loading up a real subset of Wikipedia data using the `subset-small.tsv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e44d43e-3540-4b91-ae9a-2c223e60f30c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-03T12:00:50.777006Z",
     "iopub.status.busy": "2023-05-03T12:00:50.776621Z",
     "iopub.status.idle": "2023-05-03T12:02:03.465286Z",
     "shell.execute_reply": "2023-05-03T12:02:03.463959Z",
     "shell.execute_reply.started": "2023-05-03T12:00:50.776970Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rawdata = spark.read.options(sep=\"\\t\").csv(\"/notebooks/2.data_analysis/subset-small.tsv\")\n",
    "rawdata.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3bf8ded9-25a3-490b-9470-6fbcaf581a08",
   "metadata": {},
   "source": [
    "Seems there are no column names in our source data, so we'll have to add them ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86430357-08a3-4b6a-89e3-0baef6931362",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-03T12:02:07.116011Z",
     "iopub.status.busy": "2023-05-03T12:02:07.115613Z",
     "iopub.status.idle": "2023-05-03T12:02:08.467078Z",
     "shell.execute_reply": "2023-05-03T12:02:08.465746Z",
     "shell.execute_reply.started": "2023-05-03T12:02:07.115977Z"
    }
   },
   "outputs": [],
   "source": [
    "articles = rawdata.toDF(\"ID\", \"Title\", \"Time\", \"Document\")\n",
    "articles.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76e98d68-381a-442b-b276-73cbeadade3e",
   "metadata": {},
   "source": [
    "Next we need to \"clean\" our data. We know TF/IDF can't handle null documents, so first let's check for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32588c28-fdc4-4389-8d5d-3065c8d37a3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-03T12:03:16.638041Z",
     "iopub.status.busy": "2023-05-03T12:03:16.637448Z",
     "iopub.status.idle": "2023-05-03T12:03:32.178262Z",
     "shell.execute_reply": "2023-05-03T12:03:32.177216Z",
     "shell.execute_reply.started": "2023-05-03T12:03:16.637987Z"
    }
   },
   "outputs": [],
   "source": [
    "articles.filter(articles.Document.isNull()).count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a925d14-1225-4de1-982d-52056299549f",
   "metadata": {},
   "source": [
    "Looks like there is one null document. As there is only one and it's clearly corrupt when we look into it, we can just remove it and call it a day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc736d-fd9e-4114-9e9d-1d8d90cb985a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-03T12:05:20.246190Z",
     "iopub.status.busy": "2023-05-03T12:05:20.245818Z",
     "iopub.status.idle": "2023-05-03T12:05:33.656818Z",
     "shell.execute_reply": "2023-05-03T12:05:33.655302Z",
     "shell.execute_reply.started": "2023-05-03T12:05:20.246157Z"
    }
   },
   "outputs": [],
   "source": [
    "cleanedArticles = articles.filter(articles.Document.isNotNull())\n",
    "cleanedArticles.filter(articles.Document.isNull()).count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff5a6112-640d-4e70-9590-2c300e676f0e",
   "metadata": {},
   "source": [
    "TF/IDF wants numbers, not words. So now we need to pre-process our data before we can run any fun algorithms on it. We'll first tokenize the articles to split them up into words, and store them in a sparse vector that is now a numeric representation of the words in each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b74d52d-eca9-41eb-b771-1c20d226967b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-03T12:08:45.726269Z",
     "iopub.status.busy": "2023-05-03T12:08:45.725880Z",
     "iopub.status.idle": "2023-05-03T12:08:46.543237Z",
     "shell.execute_reply": "2023-05-03T12:08:46.541967Z",
     "shell.execute_reply.started": "2023-05-03T12:08:45.726225Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "tokenizer= Tokenizer(inputCol=\"Document\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(cleanedArticles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cff6d6-86d8-49b7-bc49-5e7e7bb70037",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-03T12:10:25.054068Z",
     "iopub.status.busy": "2023-05-03T12:10:25.053677Z",
     "iopub.status.idle": "2023-05-03T12:10:38.461628Z",
     "shell.execute_reply": "2023-05-03T12:10:38.460443Z",
     "shell.execute_reply.started": "2023-05-03T12:10:25.054034Z"
    }
   },
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "featurizedData.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe626404-a76c-4a2c-b11b-5bbf31d62af9",
   "metadata": {},
   "source": [
    "That hashing operation basically computed term frequencies for us by storing how often each hashed word occured in each article. So we have TF, but we want TF/IDF scores for every term in every document. We'll store these final scores in a new column called \"features\", which is a sparse vector containing TF/IDF scores for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5fb92c-1fd3-48d8-b86e-0a9664dd4cee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-03T12:16:22.606020Z",
     "iopub.status.busy": "2023-05-03T12:16:22.605577Z",
     "iopub.status.idle": "2023-05-03T12:16:40.028943Z",
     "shell.execute_reply": "2023-05-03T12:16:40.027728Z",
     "shell.execute_reply.started": "2023-05-03T12:16:22.605976Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f452acfe-c743-4099-9411-04ed4a19f2d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-03T12:17:09.195290Z",
     "iopub.status.busy": "2023-05-03T12:17:09.194726Z",
     "iopub.status.idle": "2023-05-03T12:17:11.590260Z",
     "shell.execute_reply": "2023-05-03T12:17:11.588403Z",
     "shell.execute_reply.started": "2023-05-03T12:17:09.195238Z"
    }
   },
   "outputs": [],
   "source": [
    "rescaledData.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6e3c521-1542-4f23-9bb7-8217403aa5ff",
   "metadata": {},
   "source": [
    "So let's use this to do a search for the term \"Gettysburg\". Again, we need numbers, not words, so the first task is to get the hash value for \"Gettysburg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18e0581-6a32-4ff8-b075-289128f77c98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-03T12:23:01.518889Z",
     "iopub.status.busy": "2023-05-03T12:23:01.518515Z",
     "iopub.status.idle": "2023-05-03T12:23:16.952465Z",
     "shell.execute_reply": "2023-05-03T12:23:16.951081Z",
     "shell.execute_reply.started": "2023-05-03T12:23:01.518855Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([StructField(\"words\", ArrayType(StringType()))])\n",
    "\n",
    "df = spark.createDataFrame(([[[\"gettysburg\"]]]), schema).toDF(\"words\")\n",
    "df.show()\n",
    "\n",
    "gettysburg = hashingTF.transform(df)\n",
    "gettysburg.show()\n",
    "\n",
    "featureVec = gettysburg.select('rawFeatures').collect()\n",
    "print(featureVec)\n",
    "\n",
    "gettysburgID = int(featureVec[0].rawFeatures.indices[0])\n",
    "print(gettysburgID)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb9ba586-90f0-4042-9605-4ec4503d217f",
   "metadata": {},
   "source": [
    "OK, we have the magic number that represents \"Gettysburg\". Now we can add another column - we'll call it \"score\" - that just extracts the TF/IDF value for Gettysburg for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c093b7e6-9e6d-4469-a11f-85ebaf8ccd07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-03T12:27:43.849224Z",
     "iopub.status.busy": "2023-05-03T12:27:43.848837Z",
     "iopub.status.idle": "2023-05-03T12:28:05.298979Z",
     "shell.execute_reply": "2023-05-03T12:28:05.297469Z",
     "shell.execute_reply.started": "2023-05-03T12:27:43.849190Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "termExtractor = udf(lambda x: float(x[gettysburgID]), FloatType())\n",
    "gettysburgDF = rescaledData.withColumn('score', termExtractor(rescaledData.features))\n",
    "\n",
    "gettysburgDF.show()\n",
    "                                                        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af4420e6-f6e2-4151-8ac1-ac35fba42040",
   "metadata": {},
   "source": [
    "Now all we have to do is sort our articles by score, and we'll have the most relevant articles for Gettysburg!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a93a5a-f2e3-48b9-b216-000b8361eee4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-03T12:30:20.927477Z",
     "iopub.status.busy": "2023-05-03T12:30:20.927087Z",
     "iopub.status.idle": "2023-05-03T12:30:42.368695Z",
     "shell.execute_reply": "2023-05-03T12:30:42.366869Z",
     "shell.execute_reply.started": "2023-05-03T12:30:20.927443Z"
    }
   },
   "outputs": [],
   "source": [
    "sortedResults = gettysburgDF.filter(\"score > 0\").orderBy('score', ascending=False).select('ID', 'Title', 'Document', 'score')\n",
    "sortedResults.show(truncate=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
